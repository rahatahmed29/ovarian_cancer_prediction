{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81764c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from mealpy.swarm_based.WOA import OriginalWOA\n",
    "from mealpy.utils.problem import Problem\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Cell 2: StableFeatureSelector class definition\n",
    "class StableFeatureSelector:\n",
    "    \"\"\"\n",
    "    A feature selector that prioritizes both accuracy AND feature stability.\n",
    "    \n",
    "    Feature Stability = How consistently the same features are selected across \n",
    "    different data splits/runs. High stability means robust, reliable features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_features=15, stability_runs=5):\n",
    "        self.target_features = target_features\n",
    "        self.stability_runs = stability_runs\n",
    "        self.feature_stability_scores = {}\n",
    "        self.all_run_results = []\n",
    "        \n",
    "    def calculate_stability_metrics(self, all_selected_features):\n",
    "        \"\"\"\n",
    "        Calculate various stability metrics for feature selection\n",
    "        \n",
    "        Args:\n",
    "            all_selected_features: List of sets, each containing selected feature indices\n",
    "        \n",
    "        Returns:\n",
    "            stability_score: Overall stability score (0-1, higher is better)\n",
    "            feature_frequencies: How often each feature was selected\n",
    "        \"\"\"\n",
    "        # Count how many times each feature was selected\n",
    "        feature_counter = Counter()\n",
    "        total_selections = len(all_selected_features)\n",
    "        \n",
    "        for feature_set in all_selected_features:\n",
    "            for feature in feature_set:\n",
    "                feature_counter[feature] += 1\n",
    "        \n",
    "        # Calculate frequency of each feature (0-1 scale)\n",
    "        feature_frequencies = {feature: count/total_selections \n",
    "                             for feature, count in feature_counter.items()}\n",
    "        \n",
    "        # Jaccard Stability Index: Average pairwise Jaccard similarity\n",
    "        jaccard_scores = []\n",
    "        for i in range(len(all_selected_features)):\n",
    "            for j in range(i+1, len(all_selected_features)):\n",
    "                set1, set2 = all_selected_features[i], all_selected_features[j]\n",
    "                if len(set1.union(set2)) > 0:\n",
    "                    jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "                    jaccard_scores.append(jaccard)\n",
    "        \n",
    "        stability_score = np.mean(jaccard_scores) if jaccard_scores else 0\n",
    "        \n",
    "        return stability_score, feature_frequencies\n",
    "    \n",
    "    def pre_filter_features(self, X, y, method='mutual_info', top_k=50):\n",
    "        \"\"\"Step 1: Pre-filter features using statistical methods\"\"\"\n",
    "        print(f\"üîç Step 1: Pre-filtering to top {top_k} features using {method}...\")\n",
    "        \n",
    "        if method == 'mutual_info':\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=min(top_k, X.shape[1]))\n",
    "        else:\n",
    "            selector = SelectKBest(score_func=f_classif, k=min(top_k, X.shape[1]))\n",
    "            \n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        selected_features = selector.get_support(indices=True)\n",
    "        \n",
    "        print(f\"‚úÖ Reduced from {X.shape[1]} to {len(selected_features)} features\")\n",
    "        return X.iloc[:, selected_features], selected_features\n",
    "    \n",
    "    def create_stability_aware_fitness(self, X_train, y_train, X_val, y_val, \n",
    "                                     feature_frequencies=None, stability_weight=0.1):\n",
    "        \"\"\"\n",
    "        SPEED-OPTIMIZED fitness function that considers both accuracy AND feature stability\n",
    "        \n",
    "        SPEED OPTIMIZATIONS:\n",
    "        1. Reduced CatBoost iterations: 100 ‚Üí 30 (70% time reduction)\n",
    "        2. Early stopping enabled\n",
    "        3. Pre-converted numpy arrays for faster indexing\n",
    "        4. Cached model parameters\n",
    "        \"\"\"\n",
    "        # Pre-convert to numpy for faster indexing (major speed boost)\n",
    "        X_train_np = X_train.values\n",
    "        X_val_np = X_val.values\n",
    "        y_train_np = y_train.values\n",
    "        y_val_np = y_val.values\n",
    "        \n",
    "        def fitness_function(solution):\n",
    "            selected_indices = np.where(np.array(solution[:X_train.shape[1]]) > 0.5)[0]\n",
    "            \n",
    "            if len(selected_indices) == 0:\n",
    "                return 1e6  # Penalty for no features\n",
    "            \n",
    "            # Quick feature count check (early termination for too many features)\n",
    "            if len(selected_indices) > self.target_features * 2:  # If more than 2x target\n",
    "                feature_penalty = 0.5 * (len(selected_indices) - self.target_features)\n",
    "                return 1.0 + feature_penalty  # Return high fitness without training\n",
    "            \n",
    "            # Base fitness: prediction error\n",
    "            learning_rate = max(0.01, min(0.3, solution[-2]))\n",
    "            depth = max(3, min(10, int(solution[-1])))\n",
    "            \n",
    "            try:\n",
    "                # SPEED BOOST: Reduced iterations from 100 to 30 (70% faster)\n",
    "                model = CatBoostClassifier(\n",
    "                    verbose=0, \n",
    "                    learning_rate=learning_rate, \n",
    "                    depth=depth,\n",
    "                    iterations=30,  # MAJOR SPEED IMPROVEMENT\n",
    "                    random_seed=42,\n",
    "                    early_stopping_rounds=10,  # Stop early if no improvement\n",
    "                    use_best_model=True\n",
    "                )\n",
    "                \n",
    "                # Use numpy arrays for faster indexing\n",
    "                X_train_sel = X_train_np[:, selected_indices]\n",
    "                X_val_sel = X_val_np[:, selected_indices]\n",
    "                \n",
    "                model.fit(X_train_sel, y_train_np, \n",
    "                         eval_set=(X_val_sel, y_val_np),  # Enable early stopping\n",
    "                         verbose=False)\n",
    "                preds = model.predict(X_val_sel)\n",
    "                accuracy = accuracy_score(y_val_np, preds)\n",
    "                \n",
    "                base_fitness = 1 - accuracy  # Error to minimize\n",
    "                \n",
    "                # Feature count penalty (encourage fewer features)\n",
    "                feature_penalty = 0.1 * max(0, len(selected_indices) - self.target_features)\n",
    "                \n",
    "                # Stability bonus (encourage previously stable features)\n",
    "                stability_bonus = 0\n",
    "                if feature_frequencies is not None:\n",
    "                    avg_frequency = np.mean([feature_frequencies.get(idx, 0) \n",
    "                                           for idx in selected_indices])\n",
    "                    stability_bonus = -stability_weight * avg_frequency  # Negative = bonus\n",
    "                \n",
    "                total_fitness = base_fitness + feature_penalty + stability_bonus\n",
    "                return total_fitness\n",
    "                \n",
    "            except Exception:\n",
    "                return 1e6\n",
    "        \n",
    "        return fitness_function\n",
    "    \n",
    "    def run_single_stability_experiment(self, X_filtered, y, pre_selected_indices, \n",
    "                                      feature_frequencies=None, run_number=1):\n",
    "        \"\"\"SPEED-OPTIMIZED: Run one complete feature selection experiment with cross-validation\"\"\"\n",
    "        print(f\"\\nüß™ Stability Run {run_number} [SPEED-OPTIMIZED]...\")\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + run_number)\n",
    "        \n",
    "        run_selected_features = []\n",
    "        run_accuracies = []\n",
    "        \n",
    "        # Pre-convert to numpy arrays once (major speed boost for all folds)\n",
    "        X_filtered_np = X_filtered.values\n",
    "        y_np = y.values\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X_filtered, y), 1):\n",
    "            print(f\"      Processing Fold {fold}/5...\", end=\" \")\n",
    "            fold_start_time = pd.Timestamp.now()\n",
    "            \n",
    "            # Use numpy indexing for faster data splitting\n",
    "            X_train_full = X_filtered.iloc[train_idx]\n",
    "            y_train_full = y.iloc[train_idx]\n",
    "            X_test_fold = X_filtered.iloc[test_idx]\n",
    "            y_test_fold = y.iloc[test_idx]\n",
    "            \n",
    "            # Further split training data for WOA optimization\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train_full, y_train_full, test_size=0.3, \n",
    "                random_state=42, stratify=y_train_full\n",
    "            )\n",
    "            \n",
    "            # Create stability-aware fitness function\n",
    "            fitness_func = self.create_stability_aware_fitness(\n",
    "                X_train, y_train, X_val, y_val, \n",
    "                feature_frequencies=feature_frequencies,\n",
    "                stability_weight=0.15  # 15% weight for stability\n",
    "            )\n",
    "            \n",
    "            # Setup optimization problem\n",
    "            dim = X_filtered.shape[1] + 2\n",
    "            lb = [0] * X_filtered.shape[1] + [0.01, 3]\n",
    "            ub = [0.6] * X_filtered.shape[1] + [0.3, 10]  # Lower threshold for features\n",
    "            \n",
    "            problem = Problem(fit_func=fitness_func, lb=lb, ub=ub, \n",
    "                            minmax=\"min\", verbose=False)\n",
    "            \n",
    "            # Run optimization (keeping original epoch=15, pop_size=25 as requested)\n",
    "            optimizer = OriginalWOA(epoch=15, pop_size=25)\n",
    "            best_solution, best_fitness = optimizer.solve(problem)\n",
    "            \n",
    "            # Extract selected features\n",
    "            selected_mask = np.array(best_solution[:X_filtered.shape[1]]) > 0.5\n",
    "            selected_indices = np.where(selected_mask)[0]\n",
    "            original_indices = pre_selected_indices[selected_indices]\n",
    "            \n",
    "            # Evaluate fold performance with speed optimization\n",
    "            if len(selected_indices) > 0:\n",
    "                fold_model = CatBoostClassifier(\n",
    "                    learning_rate=best_solution[-2], depth=int(best_solution[-1]),\n",
    "                    verbose=0, \n",
    "                    iterations=50,  # Reduced from 100 to 50 for final evaluation\n",
    "                    random_seed=42,\n",
    "                    early_stopping_rounds=15\n",
    "                )\n",
    "                \n",
    "                # Use numpy arrays for faster training\n",
    "                X_train_sel = X_train_full.iloc[:, selected_indices]\n",
    "                X_test_sel = X_test_fold.iloc[:, selected_indices]\n",
    "                \n",
    "                fold_model.fit(X_train_sel, y_train_full)\n",
    "                fold_preds = fold_model.predict(X_test_sel)\n",
    "                fold_acc = accuracy_score(y_test_fold, fold_preds)\n",
    "                \n",
    "                run_selected_features.append(set(original_indices))\n",
    "                run_accuracies.append(fold_acc)\n",
    "                \n",
    "                fold_time = (pd.Timestamp.now() - fold_start_time).total_seconds()\n",
    "                print(f\"{len(selected_indices)} features, Acc: {fold_acc:.3f}, Time: {fold_time:.1f}s\")\n",
    "            else:\n",
    "                fold_time = (pd.Timestamp.now() - fold_start_time).total_seconds()\n",
    "                print(f\"No features selected, Time: {fold_time:.1f}s\")\n",
    "        \n",
    "        avg_accuracy = np.mean(run_accuracies) if run_accuracies else 0\n",
    "        print(f\"   Run {run_number} Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        \n",
    "        return run_selected_features, avg_accuracy\n",
    "\n",
    "    \n",
    "\n",
    "    def run_full_stability_selection(self, X_filtered, y, pre_selected_indices,\n",
    "                                     n_repeats=10, min_frequency=0.6):\n",
    "        \"\"\"Run multiple stability experiments with bias and final fine-tuning.\"\"\"\n",
    "        all_selected_sets = []\n",
    "        all_accuracies = []\n",
    "        feature_names = X_filtered.columns.tolist()\n",
    "        feature_frequencies = None\n",
    "        stable_feature_indices = None\n",
    "\n",
    "        for run_number in range(1, n_repeats + 1):\n",
    "            # Fine-tuning stage for last two runs\n",
    "            if run_number > n_repeats - 2 and stable_feature_indices is not None:\n",
    "                run_selected_features, avg_acc = self.run_single_stability_experiment(\n",
    "                    X_filtered, y, pre_selected_indices,\n",
    "                    feature_frequencies=None,\n",
    "                    run_number=run_number,\n",
    "                    only_use_features=stable_feature_indices\n",
    "                )\n",
    "            else:\n",
    "                run_selected_features, avg_acc = self.run_single_stability_experiment(\n",
    "                    X_filtered, y, pre_selected_indices,\n",
    "                    feature_frequencies=feature_frequencies,\n",
    "                    run_number=run_number\n",
    "                )\n",
    "\n",
    "            all_selected_sets.extend(run_selected_features)\n",
    "            all_accuracies.append(avg_acc)\n",
    "\n",
    "            # Update frequency counts\n",
    "            flat_features = [f for run_set in all_selected_sets for f in run_set]\n",
    "            counts = Counter(flat_features)\n",
    "            feature_frequencies = {feat: counts[feat] / len(all_selected_sets) \n",
    "                                   for feat in counts}\n",
    "\n",
    "            # Update stable features set\n",
    "            freq_df_temp = pd.DataFrame({\n",
    "                \"Feature Index\": list(counts.keys()),\n",
    "                \"Frequency\": list(counts.values())\n",
    "            })\n",
    "            freq_df_temp[\"Frequency (%)\"] = freq_df_temp[\"Frequency\"] / (run_number * 5)\n",
    "            stable_feature_indices = freq_df_temp[\n",
    "                freq_df_temp[\"Frequency (%)\"] >= min_frequency\n",
    "            ][\"Feature Index\"].tolist()\n",
    "\n",
    "        # Final stability frequency table\n",
    "        flat_features = [f for run_set in all_selected_sets for f in run_set]\n",
    "        counts = Counter(flat_features)\n",
    "        freq_df = pd.DataFrame({\n",
    "            \"Feature Index\": list(counts.keys()),\n",
    "            \"Frequency\": list(counts.values())\n",
    "        })\n",
    "        freq_df[\"Feature Name\"] = freq_df[\"Feature Index\"].apply(lambda idx: feature_names[idx])\n",
    "        freq_df[\"Frequency (%)\"] = freq_df[\"Frequency\"] / (n_repeats * 5)\n",
    "\n",
    "        # Final stable features\n",
    "        stable_features_df = freq_df[freq_df[\"Frequency (%)\"] >= min_frequency]\n",
    "\n",
    "        print(\"\\nüèÜ Stable Features (‚â• {:.0%} frequency):\".format(min_frequency))\n",
    "        print(stable_features_df[[\"Feature Name\", \"Frequency (%)\"]])\n",
    "        print(\"\\nAverage Accuracy across runs: {:.4f}\".format(np.mean(all_accuracies)))\n",
    "\n",
    "        return stable_features_df, freq_df\n",
    "    \n",
    "    def run_stability_analysis(self, X, y):\n",
    "        \"\"\"\n",
    "        Main method: Run multiple stability experiments and analyze results\n",
    "        \n",
    "        This is the complete procedure:\n",
    "        1. Pre-filter features to reduce search space\n",
    "        2. Run multiple independent feature selection experiments\n",
    "        3. Calculate stability metrics across all runs\n",
    "        4. Select final features based on both performance and stability\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Starting Comprehensive Feature Stability Analysis...\")\n",
    "        print(f\"Target: {self.target_features} features across {self.stability_runs} stability runs\")\n",
    "        \n",
    "        # Step 1: Pre-filter features\n",
    "        X_filtered, pre_selected_indices = self.pre_filter_features(\n",
    "            X, y, method='mutual_info', top_k=50\n",
    "        )\n",
    "        \n",
    "        # Step 2: Multiple stability runs\n",
    "        all_experiments_features = []\n",
    "        all_experiments_accuracies = []\n",
    "        \n",
    "        # First run without stability bias (baseline)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 1: Baseline Feature Selection (No Stability Bias)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        run_features, run_acc = self.run_single_stability_experiment(\n",
    "            X_filtered, y, pre_selected_indices, feature_frequencies=None, run_number=1\n",
    "        )\n",
    "        all_experiments_features.extend(run_features)\n",
    "        all_experiments_accuracies.append(run_acc)\n",
    "        \n",
    "        # Calculate initial stability metrics\n",
    "        if len(all_experiments_features) > 1:\n",
    "            stability_score, feature_frequencies = self.calculate_stability_metrics(all_experiments_features)\n",
    "            print(f\"\\nüìä After Run 1 - Stability Score: {stability_score:.3f}\")\n",
    "        else:\n",
    "            feature_frequencies = None\n",
    "        \n",
    "        # Subsequent runs with stability awareness\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 2: Stability-Aware Feature Selection\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for run in range(2, self.stability_runs + 1):\n",
    "            run_features, run_acc = self.run_single_stability_experiment(\n",
    "                X_filtered, y, pre_selected_indices, \n",
    "                feature_frequencies=feature_frequencies, run_number=run\n",
    "            )\n",
    "        \n",
    "\n",
    "            all_experiments_features.extend(run_features)\n",
    "            all_experiments_accuracies.append(run_acc)\n",
    "            \n",
    "            # Update stability metrics\n",
    "            stability_score, feature_frequencies = self.calculate_stability_metrics(all_experiments_features)\n",
    "            print(f\"üìä After Run {run} - Stability Score: {stability_score:.3f}\")\n",
    "        \n",
    "        # Step 3: Final stability analysis\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 3: Final Stability Analysis & Feature Selection\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        final_stability, final_frequencies = self.calculate_stability_metrics(all_experiments_features)\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL RESULTS:\")\n",
    "        print(f\"Overall Stability Score: {final_stability:.4f}\")\n",
    "        print(f\"Average Accuracy Across Runs: {np.mean(all_experiments_accuracies):.4f} ¬± {np.std(all_experiments_accuracies):.4f}\")\n",
    "        \n",
    "        # Step 4: Select most stable features\n",
    "        stable_features = self.select_stable_features(final_frequencies, X.columns,min_frequency=0.2)\n",
    "        \n",
    "        return stable_features, final_stability, all_experiments_accuracies\n",
    "    \n",
    "    def select_stable_features(self, feature_frequencies, feature_names, min_frequency=0.3):\n",
    "        \"\"\"\n",
    "        Select final features based on stability criteria\n",
    "        \n",
    "        Args:\n",
    "            min_frequency: Minimum frequency for a feature to be considered stable\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Feature Stability Analysis:\")\n",
    "        print(f\"Minimum frequency threshold: {min_frequency}\")\n",
    "        \n",
    "        # Sort features by frequency\n",
    "        sorted_features = sorted(feature_frequencies.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Select features above threshold, up to target number\n",
    "        stable_features = []\n",
    "        print(f\"\\nüìä Feature Stability Rankings:\")\n",
    "        print(\"Rank | Feature | Frequency | Status\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (feature_idx, frequency) in enumerate(sorted_features, 1):\n",
    "            feature_name = feature_names[feature_idx]\n",
    "            status = \"SELECTED\" if (frequency >= min_frequency and \n",
    "                                  len(stable_features) < self.target_features) else \"EXCLUDED\"\n",
    "            \n",
    "            if status == \"SELECTED\":\n",
    "                stable_features.append(feature_idx)\n",
    "            \n",
    "            print(f\"{i:4d} | {feature_name[:20]:20s} | {frequency:8.3f} | {status}\")\n",
    "            \n",
    "            if i >= 30:  # Show top 30 for brevity\n",
    "                remaining = len(sorted_features) - 30\n",
    "                if remaining > 0:\n",
    "                    print(f\"... and {remaining} more features\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected {len(stable_features)} stable features\")\n",
    "        return stable_features\n",
    "    \n",
    "    def evaluate_final_model(self, X, y, selected_features):\n",
    "        \"\"\"SPEED-OPTIMIZED: Train and evaluate final model with selected stable features\"\"\"\n",
    "        print(f\"\\nüèÜ Final Model Evaluation with {len(selected_features)} Stable Features:\")\n",
    "        \n",
    "        X_final = X.iloc[:, selected_features]\n",
    "        feature_names = X.columns[selected_features].tolist()\n",
    "        \n",
    "        print(\"Selected Features:\")\n",
    "        for i, name in enumerate(feature_names, 1):\n",
    "            print(f\"{i:2d}. {name}\")\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_final, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # SPEED-OPTIMIZED: Train final model with reduced iterations\n",
    "        final_model = CatBoostClassifier(\n",
    "            learning_rate=0.1, \n",
    "            depth=6, \n",
    "            verbose=0, \n",
    "            iterations=100,  # Reduced from 200 to 100 for speed\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=20,  # Add early stopping\n",
    "            use_best_model=True\n",
    "        )\n",
    "        \n",
    "        # Use eval_set for early stopping\n",
    "        final_model.fit(X_train, y_train, \n",
    "                       eval_set=(X_test, y_test),\n",
    "                       verbose=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_preds = final_model.predict(X_train)\n",
    "        test_preds = final_model.predict(X_test)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_preds)\n",
    "        test_acc = accuracy_score(y_test, test_preds)\n",
    "        \n",
    "        print(f\"\\nüìà Final Model Performance:\")\n",
    "        print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Generalization: {test_acc - train_acc:.4f} (closer to 0 is better)\")\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = final_model.get_feature_importance()\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüèÖ Top 10 Most Important Features:\")\n",
    "        print(importance_df.head(10).to_string(index=False))\n",
    "        \n",
    "        return final_model, test_acc, importance_df\n",
    "\n",
    "# Cell 3: Main function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    COMPLETE PROCEDURE EXPLANATION:\n",
    "    \n",
    "    1. LOAD DATA: Read your dataset\n",
    "    2. PRE-FILTERING: Use statistical methods to reduce feature space\n",
    "    3. STABILITY RUNS: Run feature selection multiple times with different random seeds\n",
    "    4. STABILITY MEASUREMENT: Calculate how consistently features are selected\n",
    "    5. STABLE FEATURE SELECTION: Choose features that appear frequently across runs\n",
    "    6. FINAL EVALUATION: Train final model and report performance\n",
    "    \n",
    "    WHY THIS MATTERS FOR YOUR THESIS:\n",
    "    - Stable features are more likely to generalize to new data\n",
    "    - Reviewers will appreciate the robustness of your feature selection\n",
    "    - You can confidently claim your features are reliable, not just lucky\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéì STABLE FEATURE SELECTION FOR THESIS RESEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load your data\n",
    "    df = pd.read_csv(\".././../data/processed/cleaned_data.csv\")\n",
    "    X = df.drop(columns=[\"TYPE\"])\n",
    "    y = df[\"TYPE\"]\n",
    "    \n",
    "    print(f\"üìä Dataset Info:\")\n",
    "    print(f\"Samples: {X.shape[0]}, Original Features: {X.shape[1]}\")\n",
    "    print(f\"Target Classes: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Initialize stable feature selector\n",
    "    selector = StableFeatureSelector(target_features=10, stability_runs=5)  # 3 runs for demo\n",
    "    \n",
    "    # Run complete stability analysis\n",
    "    stable_features, stability_score, accuracies = selector.run_stability_analysis(X, y)\n",
    "    \n",
    "    # Evaluate final model\n",
    "    final_model, test_accuracy, importance_df = selector.evaluate_final_model(X, y, stable_features)\n",
    "    \n",
    "    # Summary for thesis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìù THESIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Selected Features: {len(stable_features)}\")\n",
    "    print(f\"‚úÖ Feature Stability Score: {stability_score:.4f} (0-1 scale, higher is better)\")\n",
    "    print(f\"‚úÖ Cross-validation Accuracy: {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")\n",
    "    print(f\"‚úÖ Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"‚úÖ Method: Stability-aware Whale Optimization Algorithm\")\n",
    "    \n",
    "    # What to report in your thesis\n",
    "    print(f\"\\nüìñ FOR YOUR THESIS PAPER:\")\n",
    "    print(f\"1. Method: 'We used a stability-aware feature selection approach'\")\n",
    "    print(f\"2. Stability: 'Features were selected based on consistency across {selector.stability_runs} independent runs'\")\n",
    "    print(f\"3. Results: '{len(stable_features)} stable features achieved {test_accuracy:.1%} accuracy'\")\n",
    "    print(f\"4. Robustness: 'Stability score of {stability_score:.3f} indicates reliable feature selection'\")\n",
    "    \n",
    "    return stable_features, final_model, stability_score\n",
    "# Cell 4: Execute main function (if running as script)\n",
    "if __name__ == \"__main__\":\n",
    "    selected_features, model, stability = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
